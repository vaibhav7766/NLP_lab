{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:49:43.806359Z","iopub.status.busy":"2024-10-14T20:49:43.805987Z","iopub.status.idle":"2024-10-14T20:50:01.899476Z","shell.execute_reply":"2024-10-14T20:50:01.898340Z","shell.execute_reply.started":"2024-10-14T20:49:43.806317Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using CPU\n","Device: cpu\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\vaibh\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to\n","[nltk_data]     C:\\Users\\vaibh\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from rouge import Rouge\n","import os\n","from collections import Counter\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download(\"punkt\")\n","nltk.download(\"punkt_tab\")\n","\n","def get_device():\n","    if torch.cuda.is_available():\n","        print(\"Using GPU\")\n","        return torch.device('cuda')\n","    else:\n","        print(\"Using CPU\")\n","        return torch.device('cpu')\n","\n","device = get_device()\n","\n","print(f'Device: {device}')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:50:01.901639Z","iopub.status.busy":"2024-10-14T20:50:01.901148Z","iopub.status.idle":"2024-10-14T20:50:01.913894Z","shell.execute_reply":"2024-10-14T20:50:01.912864Z","shell.execute_reply.started":"2024-10-14T20:50:01.901600Z"},"trusted":true},"outputs":[],"source":["class BiLSTMSummarizer(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super(BiLSTMSummarizer, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n","        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        batch_size = src.shape[0]\n","        trg_len = trg.shape[1]\n","        trg_vocab_size = self.fc.out_features\n","\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n","\n","        embedded = self.embedding(src)\n","        enc_output, (hidden, cell) = self.encoder(embedded)\n","\n","        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1).unsqueeze(0)\n","        cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1).unsqueeze(0)\n","\n","        input = trg[:, 0]\n","\n","        for t in range(1, trg_len):\n","            input_embedded = self.embedding(input).unsqueeze(1)\n","            output, (hidden, cell) = self.decoder(input_embedded, (hidden, cell))\n","            prediction = self.fc(output.squeeze(1))\n","            outputs[:, t] = prediction\n","\n","            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n","            top1 = prediction.argmax(1)\n","            input = trg[:, t] if teacher_force else top1\n","\n","        return outputs"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:50:01.915831Z","iopub.status.busy":"2024-10-14T20:50:01.915237Z","iopub.status.idle":"2024-10-14T20:50:01.930258Z","shell.execute_reply":"2024-10-14T20:50:01.929408Z","shell.execute_reply.started":"2024-10-14T20:50:01.915786Z"},"trusted":true},"outputs":[],"source":["# Custom dataset class\n","class SummarizationDataset(Dataset):\n","    def __init__(self, articles, summaries, vocab, max_length=100):\n","        self.articles = articles\n","        self.summaries = summaries\n","        self.vocab = vocab\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.articles)\n","\n","    def __getitem__(self, idx):\n","        article = self.articles[idx]\n","        summary = self.summaries[idx]\n","\n","        article_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article][:self.max_length-2] + [self.vocab['<eos>']]\n","        summary_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary][:self.max_length-2] + [self.vocab['<eos>']]\n","\n","        article_indices = article_indices + [self.vocab['<pad>']] * (self.max_length - len(article_indices))\n","        summary_indices = summary_indices + [self.vocab['<pad>']] * (self.max_length - len(summary_indices))\n","\n","        return torch.tensor(article_indices), torch.tensor(summary_indices)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:50:01.932958Z","iopub.status.busy":"2024-10-14T20:50:01.932675Z","iopub.status.idle":"2024-10-14T20:50:01.945016Z","shell.execute_reply":"2024-10-14T20:50:01.944103Z","shell.execute_reply.started":"2024-10-14T20:50:01.932928Z"},"trusted":true},"outputs":[],"source":["def load_data(file_path):\n","    df = pd.read_csv(file_path)\n","    articles = df['Content'].tolist()   # Use the 'Content' as the source article\n","    summaries = df['Headline'].tolist() # Use the 'Headline' as the target summary\n","    return articles, summaries"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:50:01.946849Z","iopub.status.busy":"2024-10-14T20:50:01.946194Z","iopub.status.idle":"2024-10-14T20:50:01.955636Z","shell.execute_reply":"2024-10-14T20:50:01.954844Z","shell.execute_reply.started":"2024-10-14T20:50:01.946804Z"},"trusted":true},"outputs":[],"source":["# Tokenize text\n","def tokenize(text):\n","    return word_tokenize(text.lower())"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:50:01.957125Z","iopub.status.busy":"2024-10-14T20:50:01.956820Z","iopub.status.idle":"2024-10-14T20:50:01.966310Z","shell.execute_reply":"2024-10-14T20:50:01.965375Z","shell.execute_reply.started":"2024-10-14T20:50:01.957085Z"},"trusted":true},"outputs":[],"source":["# Build vocabulary\n","def build_vocab(texts, min_freq=2):\n","    word_freq = Counter()\n","    for text in texts:\n","        word_freq.update(text)\n","\n","    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n","    for word, freq in word_freq.items():\n","        if freq >= min_freq:\n","            vocab[word] = len(vocab)\n","\n","    return vocab, {v: k for k, v in vocab.items()}"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:50:01.967558Z","iopub.status.busy":"2024-10-14T20:50:01.967276Z","iopub.status.idle":"2024-10-14T20:50:07.046143Z","shell.execute_reply":"2024-10-14T20:50:07.045355Z","shell.execute_reply.started":"2024-10-14T20:50:01.967517Z"},"trusted":true},"outputs":[],"source":["# Load data\n","articles, summaries = load_data(r'hindi_news_dataset.csv')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:50:07.047586Z","iopub.status.busy":"2024-10-14T20:50:07.047266Z","iopub.status.idle":"2024-10-14T20:52:34.307858Z","shell.execute_reply":"2024-10-14T20:52:34.306792Z","shell.execute_reply.started":"2024-10-14T20:50:07.047554Z"},"trusted":true},"outputs":[],"source":["# Tokenize data\n","tokenized_articles = [tokenize(article) for article in articles]\n","tokenized_summaries = [tokenize(summary) for summary in summaries]"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:52:34.309583Z","iopub.status.busy":"2024-10-14T20:52:34.309192Z","iopub.status.idle":"2024-10-14T20:52:37.156801Z","shell.execute_reply":"2024-10-14T20:52:37.156010Z","shell.execute_reply.started":"2024-10-14T20:52:34.309538Z"},"trusted":true},"outputs":[],"source":["# Build vocabulary\n","vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:52:37.161010Z","iopub.status.busy":"2024-10-14T20:52:37.160707Z","iopub.status.idle":"2024-10-14T20:52:37.372593Z","shell.execute_reply":"2024-10-14T20:52:37.371729Z","shell.execute_reply.started":"2024-10-14T20:52:37.160980Z"},"trusted":true},"outputs":[],"source":["# Split data\n","train_articles, test_articles, train_summaries, test_summaries = train_test_split(tokenized_articles, tokenized_summaries, test_size=0.2, random_state=42)\n","train_articles, val_articles, train_summaries, val_summaries = train_test_split(train_articles, train_summaries, test_size=0.1, random_state=42)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:52:37.374140Z","iopub.status.busy":"2024-10-14T20:52:37.373784Z","iopub.status.idle":"2024-10-14T20:52:37.393668Z","shell.execute_reply":"2024-10-14T20:52:37.392723Z","shell.execute_reply.started":"2024-10-14T20:52:37.374100Z"},"trusted":true},"outputs":[],"source":["train_dataset = SummarizationDataset(train_articles, train_summaries, vocab)\n","val_dataset = SummarizationDataset(val_articles, val_summaries, vocab)\n","test_dataset = SummarizationDataset(test_articles, test_summaries, vocab)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:52:37.395039Z","iopub.status.busy":"2024-10-14T20:52:37.394757Z","iopub.status.idle":"2024-10-14T20:52:37.404926Z","shell.execute_reply":"2024-10-14T20:52:37.404007Z","shell.execute_reply.started":"2024-10-14T20:52:37.395008Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:52:37.406260Z","iopub.status.busy":"2024-10-14T20:52:37.405969Z","iopub.status.idle":"2024-10-14T20:52:38.091419Z","shell.execute_reply":"2024-10-14T20:52:38.090568Z","shell.execute_reply.started":"2024-10-14T20:52:37.406229Z"},"trusted":true},"outputs":[],"source":["model = BiLSTMSummarizer(len(vocab), embedding_dim=128, hidden_dim=256, output_dim=len(vocab)).to(device)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:52:38.092798Z","iopub.status.busy":"2024-10-14T20:52:38.092482Z","iopub.status.idle":"2024-10-14T20:52:38.101501Z","shell.execute_reply":"2024-10-14T20:52:38.100462Z","shell.execute_reply.started":"2024-10-14T20:52:38.092765Z"},"trusted":true},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, device, clip=1, teacher_forcing_ratio=0.5):\n","    model.train()\n","    epoch_loss = 0\n","    for batch in tqdm(iterator, desc=\"Training\"):\n","        src, trg = batch\n","        src, trg = src.to(device), trg.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(src, trg, teacher_forcing_ratio)\n","\n","        output_dim = output.shape[-1]\n","        output = output[:, 1:].reshape(-1, output_dim)\n","        trg = trg[:, 1:].reshape(-1)\n","\n","        loss = criterion(output, trg)\n","        loss.backward()\n","\n","        if isinstance(device, torch.device) and device.type == 'xla':\n","            xm.optimizer_step(optimizer, barrier=True)  # TPU-specific optimizer step\n","        else:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:52:38.102933Z","iopub.status.busy":"2024-10-14T20:52:38.102647Z","iopub.status.idle":"2024-10-14T20:52:38.111674Z","shell.execute_reply":"2024-10-14T20:52:38.110851Z","shell.execute_reply.started":"2024-10-14T20:52:38.102901Z"},"trusted":true},"outputs":[],"source":["# Evaluation function\n","def evaluate(model, iterator, criterion, device):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for batch in tqdm(iterator, desc=\"Evaluating\"):\n","            src, trg = batch\n","            src, trg = src.to(device), trg.to(device)\n","\n","            output = model(src, trg, 0)  # turn off teacher forcing\n","\n","            output_dim = output.shape[-1]\n","            output = output[:, 1:].reshape(-1, output_dim)\n","            trg = trg[:, 1:].reshape(-1)\n","\n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:52:38.112898Z","iopub.status.busy":"2024-10-14T20:52:38.112622Z","iopub.status.idle":"2024-10-14T20:52:38.963402Z","shell.execute_reply":"2024-10-14T20:52:38.962399Z","shell.execute_reply.started":"2024-10-14T20:52:38.112868Z"},"trusted":true},"outputs":[],"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:52:38.965226Z","iopub.status.busy":"2024-10-14T20:52:38.964668Z","iopub.status.idle":"2024-10-15T05:01:25.764345Z","shell.execute_reply":"2024-10-15T05:01:25.763159Z","shell.execute_reply.started":"2024-10-14T20:52:38.965181Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 4174/4174 [1:17:12<00:00,  1.11s/it]\n","Evaluating: 100%|██████████| 464/464 [04:24<00:00,  1.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01\n","\tTrain Loss: 4.964\n","\t Val. Loss: 4.480\n","Model saved to 'best_model.pth'\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 4174/4174 [1:17:03<00:00,  1.11s/it]\n","Evaluating: 100%|██████████| 464/464 [04:22<00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 02\n","\tTrain Loss: 2.686\n","\t Val. Loss: 3.318\n","Model saved to 'best_model.pth'\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 4174/4174 [1:17:08<00:00,  1.11s/it]\n","Evaluating: 100%|██████████| 464/464 [04:22<00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 03\n","\tTrain Loss: 1.846\n","\t Val. Loss: 2.781\n","Model saved to 'best_model.pth'\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 4174/4174 [1:17:04<00:00,  1.11s/it]\n","Evaluating: 100%|██████████| 464/464 [04:22<00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 04\n","\tTrain Loss: 1.408\n","\t Val. Loss: 2.447\n","Model saved to 'best_model.pth'\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 4174/4174 [1:17:03<00:00,  1.11s/it]\n","Evaluating: 100%|██████████| 464/464 [04:22<00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 05\n","\tTrain Loss: 1.139\n","\t Val. Loss: 2.214\n","Model saved to 'best_model.pth'\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 4174/4174 [1:16:56<00:00,  1.11s/it]\n","Evaluating: 100%|██████████| 464/464 [04:21<00:00,  1.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 06\n","\tTrain Loss: 0.957\n","\t Val. Loss: 2.059\n","Model saved to 'best_model.pth'\n"]}],"source":["num_epochs = 6\n","best_val_loss = float('inf')\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, optimizer, criterion, device)\n","    val_loss = evaluate(model, val_loader, criterion, device)\n","    print(f'Epoch: {epoch+1:02}')\n","    print(f'\\tTrain Loss: {train_loss:.3f}')\n","    print(f'\\t Val. Loss: {val_loss:.3f}')\n","    # Save model if validation loss improves\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save({'model_state_dict': model.state_dict(), 'vocab': vocab}, 'best_model.pth')\n","        print(f\"Model saved to 'best_model.pth'\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T05:01:25.766017Z","iopub.status.busy":"2024-10-15T05:01:25.765678Z","iopub.status.idle":"2024-10-15T05:01:25.771793Z","shell.execute_reply":"2024-10-15T05:01:25.770729Z","shell.execute_reply.started":"2024-10-15T05:01:25.765982Z"},"trusted":true},"outputs":[],"source":["# Load model function\n","def load_model(filepath, device):\n","    checkpoint = torch.load(filepath, map_location=device)\n","    vocab = checkpoint['vocab']\n","    model = BiLSTMSummarizer(len(vocab), embedding_dim=128, hidden_dim=256, output_dim=len(vocab)).to(device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    return model, checkpoint"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T05:01:25.773341Z","iopub.status.busy":"2024-10-15T05:01:25.773021Z","iopub.status.idle":"2024-10-15T05:12:23.683330Z","shell.execute_reply":"2024-10-15T05:12:23.682356Z","shell.execute_reply.started":"2024-10-15T05:01:25.773308Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\vaibh\\AppData\\Local\\Temp\\ipykernel_32760\\67957849.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(filepath, map_location=device)\n","Evaluating:   0%|          | 3/1160 [00:18<1:56:41,  6.05s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the best model for testing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m best_model, _ \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n\u001b[1;32m----> 3\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[1;32mIn[7], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, iterator, criterion, device)\u001b[0m\n\u001b[0;32m      7\u001b[0m src, trg \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m      8\u001b[0m src, trg \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mto(device), trg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# turn off teacher forcing\u001b[39;00m\n\u001b[0;32m     12\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dim)\n","File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[5], line 31\u001b[0m, in \u001b[0;36mBiLSTMSummarizer.forward\u001b[1;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     28\u001b[0m     outputs[:, t] \u001b[38;5;241m=\u001b[39m prediction\n\u001b[0;32m     30\u001b[0m     teacher_force \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m teacher_forcing_ratio\n\u001b[1;32m---> 31\u001b[0m     top1 \u001b[38;5;241m=\u001b[39m \u001b[43mprediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m trg[:, t] \u001b[38;5;28;01mif\u001b[39;00m teacher_force \u001b[38;5;28;01melse\u001b[39;00m top1\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Load the best model for testing\n","best_model, _ = load_model('best_model.pth', device)\n","test_loss = evaluate(best_model, test_loader, criterion, device)\n","print(f'Test Loss: {test_loss:.3f}')"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["optimizer = optim.Adam(best_model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def beam_search(\n","    model,\n","    src,\n","    vocab,\n","    inv_vocab,\n","    beam_width=3,\n","    max_length=100,\n","    min_length=10,\n","    device=\"cpu\",\n","):\n","    model.eval()\n","    with torch.no_grad():\n","        # Embedding the input sequence\n","        embedded = model.embedding(src)  # shape: (batch_size, seq_len, embedding_dim)\n","        enc_output, (hidden, cell) = model.encoder(embedded)  # LSTM encoder output\n","\n","        # In case of bi-directional LSTM, combine the hidden states\n","        if model.encoder.bidirectional:\n","            hidden = torch.cat(\n","                (hidden[-2, :, :], hidden[-1, :, :]), dim=1\n","            )  # shape: (batch_size, hidden_dim)\n","            cell = torch.cat(\n","                (cell[-2, :, :], cell[-1, :, :]), dim=1\n","            )  # shape: (batch_size, hidden_dim)\n","        else:\n","            hidden = hidden[-1, :, :]  # Take the last layer if not bi-directional\n","            cell = cell[-1, :, :]  # Take the last layer if not bi-directional\n","\n","        # Now we process one sequence at a time, so set batch size to 1\n","        hidden = hidden.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n","        cell = cell.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n","\n","        # Initialize the beam with the start-of-sequence token\n","        beam = [\n","            ([vocab[\"<sos>\"]], 0, hidden[:, 0:1, :], cell[:, 0:1, :])\n","        ]  # Start with one sequence\n","        complete_hypotheses = []\n","\n","        # Perform beam search\n","        for t in range(max_length):\n","            new_beam = []\n","            for seq, score, hidden, cell in beam:\n","                # If end-of-sequence token is reached and length is >= min_length, add to complete hypotheses\n","                if seq[-1] == vocab[\"<eos>\"] and len(seq) >= min_length:\n","                    complete_hypotheses.append((seq, score))\n","                    continue\n","\n","                # Prepare the input for the decoder (last predicted token)\n","                input = (\n","                    torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)\n","                )  # shape: (1, 1)\n","                input_embedded = model.embedding(input)  # shape: (1, 1, embedding_dim)\n","\n","                # Pass through the decoder\n","                output, (hidden, cell) = model.decoder(\n","                    input_embedded, (hidden, cell)\n","                )  # Decode step\n","                predictions = model.fc(\n","                    output.squeeze(1)\n","                )  # Linear layer to get vocab distribution\n","\n","                # Get top beam_width predictions\n","                topk_scores, topk_indices = torch.topk(predictions, beam_width, dim=1)\n","\n","                for i in range(beam_width):\n","                    next_seq = seq + [topk_indices[0, i].item()]\n","                    next_score = score + topk_scores[0, i].item()\n","                    new_beam.append((next_seq, next_score, hidden, cell))\n","\n","            # Sort the beam by score and select the top candidates\n","            beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n","\n","        # If no complete hypotheses were found, return the highest scoring incomplete hypothesis\n","        if len(complete_hypotheses) == 0:\n","            complete_hypotheses = beam\n","\n","        # Return the sequence with the highest score\n","        best_hypothesis = max(complete_hypotheses, key=lambda x: x[1])[0]\n","        return [\n","            inv_vocab[idx]\n","            for idx in best_hypothesis\n","            if idx not in [vocab[\"<sos>\"], vocab[\"<eos>\"], vocab[\"<pad>\"]]\n","        ]"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T05:12:23.685478Z","iopub.status.busy":"2024-10-15T05:12:23.684757Z","iopub.status.idle":"2024-10-15T05:12:24.107877Z","shell.execute_reply":"2024-10-15T05:12:24.106537Z","shell.execute_reply.started":"2024-10-15T05:12:23.685429Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating summaries:   0%|          | 1/1160 [00:01<21:30,  1.11s/it]\n"]},{"ename":"KeyError","evalue":"60441","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[29], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m src, trg \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m      9\u001b[0m src \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 10\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(pred)])\n\u001b[0;32m     12\u001b[0m references\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([inv_vocab[idx\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m trg[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m'\u001b[39m], vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m'\u001b[39m], vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m]]])])\n","Cell \u001b[1;32mIn[28], line 80\u001b[0m, in \u001b[0;36mbeam_search\u001b[1;34m(model, src, vocab, inv_vocab, beam_width, max_length, min_length, device)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Return the sequence with the highest score\u001b[39;00m\n\u001b[0;32m     78\u001b[0m best_hypothesis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(complete_hypotheses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m---> 80\u001b[0m     \u001b[43minv_vocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m best_hypothesis\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m\"\u001b[39m], vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m\"\u001b[39m], vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     83\u001b[0m ]\n","\u001b[1;31mKeyError\u001b[0m: 60441"]}],"source":["# Evaluate using ROUGE score\n","rouge = Rouge()\n","best_model.eval()\n","predictions = []\n","references = []\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n","        src, trg = batch\n","        src = src.to(device)\n","        pred = beam_search(best_model, src, vocab, inv_vocab, min_length=10, device=device)\n","        predictions.extend([' '.join(pred)])\n","        references.extend([' '.join([inv_vocab[idx.item()] for idx in trg[0] if idx.item() not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-15T05:12:24.108757Z","iopub.status.idle":"2024-10-15T05:12:24.109147Z","shell.execute_reply":"2024-10-15T05:12:24.108960Z","shell.execute_reply.started":"2024-10-15T05:12:24.108941Z"},"trusted":true},"outputs":[],"source":["min_length = 10\n","predictions = [' '.join(pred[:min_length]) for pred in predictions]\n","scores = rouge.get_scores(predictions, references, avg=True)\n","print(\"ROUGE scores:\", scores)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3758343,"sourceId":9253476,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
