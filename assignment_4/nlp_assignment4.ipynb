{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhav7766/NLP_lab/blob/main/assignment_4/nlp_assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e19BGeansIUc"
      },
      "source": [
        "<h1 align=\"center\">NLP Assignment 4: Hindi News Summarization</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVEu8rPLsIUe"
      },
      "source": [
        "Vaibhav Sharma  \n",
        "AIML-B2  \n",
        "22070126125"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX50c6gRsIUe"
      },
      "source": [
        "## [Github Link](https://github.com/vaibhav7766/NLP_lab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:17:56.670556Z",
          "iopub.status.busy": "2024-10-18T04:17:56.669855Z",
          "iopub.status.idle": "2024-10-18T04:17:56.675823Z",
          "shell.execute_reply": "2024-10-18T04:17:56.674984Z",
          "shell.execute_reply.started": "2024-10-18T04:17:56.670516Z"
        },
        "trusted": true,
        "id": "iSGdCg_8sIUe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from rouge import Rouge\n",
        "import os\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FazWtu4vsIUf",
        "outputId": "32d49b6f-8285-412f-a99b-1a0877ea5f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:17:59.790438Z",
          "iopub.status.busy": "2024-10-18T04:17:59.789795Z",
          "iopub.status.idle": "2024-10-18T04:17:59.801926Z",
          "shell.execute_reply": "2024-10-18T04:17:59.800971Z",
          "shell.execute_reply.started": "2024-10-18T04:17:59.790395Z"
        },
        "trusted": true,
        "id": "oIVNripNsIUf"
      },
      "outputs": [],
      "source": [
        "# Define the BiLSTM-based summarizer model\n",
        "class BiLSTMSummarizer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMSummarizer, self).__init__()\n",
        "        # Embedding layer to convert input tokens into dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Bidirectional LSTM encoder\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # LSTM decoder\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)\n",
        "\n",
        "        # Fully connected layer to generate predictions\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.fc.out_features\n",
        "\n",
        "        # Tensor to store model predictions\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
        "\n",
        "        # Pass the input through the embedding layer\n",
        "        embedded = self.embedding(src)\n",
        "\n",
        "        # Encode the input sequence using the bidirectional LSTM encoder\n",
        "        enc_output, (hidden, cell) = self.encoder(embedded)\n",
        "\n",
        "        # Concatenate the hidden states from both directions of the LSTM\n",
        "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1).unsqueeze(0)\n",
        "\n",
        "        # Initialize the first input to the decoder (typically the <sos> token)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        # Iterate through the target sequence\n",
        "        for t in range(1, trg_len):\n",
        "            # Get the embedding for the current input token\n",
        "            input_embedded = self.embedding(input).unsqueeze(1)\n",
        "\n",
        "            # Pass the input through the LSTM decoder\n",
        "            output, (hidden, cell) = self.decoder(input_embedded, (hidden, cell))\n",
        "\n",
        "            # Generate predictions using the fully connected layer\n",
        "            prediction = self.fc(output.squeeze(1))\n",
        "            outputs[:, t] = prediction\n",
        "\n",
        "            # Apply teacher forcing: decide whether to use the true target or the predicted token\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(1)\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:18:02.449898Z",
          "iopub.status.busy": "2024-10-18T04:18:02.449459Z",
          "iopub.status.idle": "2024-10-18T04:18:02.463698Z",
          "shell.execute_reply": "2024-10-18T04:18:02.462094Z",
          "shell.execute_reply.started": "2024-10-18T04:18:02.449853Z"
        },
        "trusted": true,
        "id": "_tvAb8IqsIUf"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class for text summarization\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, articles, summaries, vocab, max_length=100):\n",
        "        self.articles = articles          # List of input articles\n",
        "        self.summaries = summaries        # Corresponding list of target summaries\n",
        "        self.vocab = vocab                # Vocabulary mapping words to indices\n",
        "        self.max_length = max_length      # Maximum length for input and output sequences\n",
        "\n",
        "    # Return the number of examples in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    # Get a single example (article and summary) by index\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        # Convert article tokens to indices, adding <sos> at the start and <eos> at the end\n",
        "        article_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "\n",
        "        # Convert summary tokens to indices, adding <sos> at the start and <eos> at the end\n",
        "        summary_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "\n",
        "        # Pad the sequences to the maximum length using the <pad> token\n",
        "        article_indices = article_indices + [self.vocab['<pad>']] * (self.max_length - len(article_indices))\n",
        "        summary_indices = summary_indices + [self.vocab['<pad>']] * (self.max_length - len(summary_indices))\n",
        "\n",
        "        # Return article and summary as PyTorch tensors\n",
        "        return torch.tensor(article_indices), torch.tensor(summary_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:18:05.397262Z",
          "iopub.status.busy": "2024-10-18T04:18:05.396528Z",
          "iopub.status.idle": "2024-10-18T04:18:05.401965Z",
          "shell.execute_reply": "2024-10-18T04:18:05.400980Z",
          "shell.execute_reply.started": "2024-10-18T04:18:05.397210Z"
        },
        "trusted": true,
        "id": "x-AoJyPosIUg"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    articles = df['Content'].tolist()   # Use the 'Content' as the source article\n",
        "    summaries = df['Headline'].tolist() # Use the 'Headline' as the target summary\n",
        "    return articles, summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:18:07.339808Z",
          "iopub.status.busy": "2024-10-18T04:18:07.338930Z",
          "iopub.status.idle": "2024-10-18T04:18:07.344068Z",
          "shell.execute_reply": "2024-10-18T04:18:07.343001Z",
          "shell.execute_reply.started": "2024-10-18T04:18:07.339766Z"
        },
        "trusted": true,
        "id": "RmPfjb6RsIUg"
      },
      "outputs": [],
      "source": [
        "# Tokenize text\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:18:11.201072Z",
          "iopub.status.busy": "2024-10-18T04:18:11.200361Z",
          "iopub.status.idle": "2024-10-18T04:18:11.206743Z",
          "shell.execute_reply": "2024-10-18T04:18:11.205935Z",
          "shell.execute_reply.started": "2024-10-18T04:18:11.201031Z"
        },
        "trusted": true,
        "id": "g9bw-JvesIUg"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary\n",
        "def build_vocab(texts, min_freq=2):\n",
        "    word_freq = Counter()\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    return vocab, {v: k for k, v in vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:18:13.543393Z",
          "iopub.status.busy": "2024-10-18T04:18:13.542645Z",
          "iopub.status.idle": "2024-10-18T04:18:17.722044Z",
          "shell.execute_reply": "2024-10-18T04:18:17.720906Z",
          "shell.execute_reply.started": "2024-10-18T04:18:13.543353Z"
        },
        "trusted": true,
        "id": "FMB6riFCsIUg"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "articles, summaries = load_data('hindi_news_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:20:10.117344Z",
          "iopub.status.busy": "2024-10-18T04:20:10.116574Z",
          "iopub.status.idle": "2024-10-18T04:22:35.416521Z",
          "shell.execute_reply": "2024-10-18T04:22:35.415736Z",
          "shell.execute_reply.started": "2024-10-18T04:20:10.117301Z"
        },
        "trusted": true,
        "id": "3_8MzexIsIUh"
      },
      "outputs": [],
      "source": [
        "# Tokenize data\n",
        "tokenized_articles = [tokenize(article) for article in articles]\n",
        "tokenized_summaries = [tokenize(summary) for summary in summaries]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:22:50.055290Z",
          "iopub.status.busy": "2024-10-18T04:22:50.054916Z",
          "iopub.status.idle": "2024-10-18T04:22:52.876263Z",
          "shell.execute_reply": "2024-10-18T04:22:52.875436Z",
          "shell.execute_reply.started": "2024-10-18T04:22:50.055256Z"
        },
        "trusted": true,
        "id": "Br7yWkkOsIUh"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary\n",
        "vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:22:58.393374Z",
          "iopub.status.busy": "2024-10-18T04:22:58.392601Z",
          "iopub.status.idle": "2024-10-18T04:22:58.595885Z",
          "shell.execute_reply": "2024-10-18T04:22:58.594987Z",
          "shell.execute_reply.started": "2024-10-18T04:22:58.393332Z"
        },
        "trusted": true,
        "id": "geUoXOj1sIUh"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "train_articles, test_articles, train_summaries, test_summaries = train_test_split(tokenized_articles, tokenized_summaries, test_size=0.2, random_state=42)\n",
        "train_articles, val_articles, train_summaries, val_summaries = train_test_split(train_articles, train_summaries, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:23:00.456548Z",
          "iopub.status.busy": "2024-10-18T04:23:00.456152Z",
          "iopub.status.idle": "2024-10-18T04:23:00.461405Z",
          "shell.execute_reply": "2024-10-18T04:23:00.460375Z",
          "shell.execute_reply.started": "2024-10-18T04:23:00.456511Z"
        },
        "trusted": true,
        "id": "PlIZQBY4sIUh"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = SummarizationDataset(train_articles, train_summaries, vocab)\n",
        "val_dataset = SummarizationDataset(val_articles, val_summaries, vocab)\n",
        "test_dataset = SummarizationDataset(test_articles, test_summaries, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:23:02.756736Z",
          "iopub.status.busy": "2024-10-18T04:23:02.755898Z",
          "iopub.status.idle": "2024-10-18T04:23:02.761754Z",
          "shell.execute_reply": "2024-10-18T04:23:02.760781Z",
          "shell.execute_reply.started": "2024-10-18T04:23:02.756691Z"
        },
        "trusted": true,
        "id": "t7SW_za_sIUh"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:23:05.389614Z",
          "iopub.status.busy": "2024-10-18T04:23:05.388655Z",
          "iopub.status.idle": "2024-10-18T04:23:06.051577Z",
          "shell.execute_reply": "2024-10-18T04:23:06.050587Z",
          "shell.execute_reply.started": "2024-10-18T04:23:05.389570Z"
        },
        "trusted": true,
        "id": "v_nPT6U9sIUh"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = BiLSTMSummarizer(len(vocab), embedding_dim=128, hidden_dim=256, output_dim=len(vocab)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:23:10.112840Z",
          "iopub.status.busy": "2024-10-18T04:23:10.111861Z",
          "iopub.status.idle": "2024-10-18T04:23:10.120209Z",
          "shell.execute_reply": "2024-10-18T04:23:10.119336Z",
          "shell.execute_reply.started": "2024-10-18T04:23:10.112795Z"
        },
        "trusted": true,
        "id": "8_-x4TmzsIUh"
      },
      "outputs": [],
      "source": [
        "# Train function\n",
        "def train(model, iterator, optimizer, criterion, device, clip=1, teacher_forcing_ratio=0.5):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(iterator, desc=\"Training\"):\n",
        "        src, trg = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg, teacher_forcing_ratio)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:23:12.574836Z",
          "iopub.status.busy": "2024-10-18T04:23:12.574076Z",
          "iopub.status.idle": "2024-10-18T04:23:12.581596Z",
          "shell.execute_reply": "2024-10-18T04:23:12.580644Z",
          "shell.execute_reply.started": "2024-10-18T04:23:12.574797Z"
        },
        "trusted": true,
        "id": "-AFBtmMnsIUh"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            src, trg = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0)  # turn off teacher forcing\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:23:16.846090Z",
          "iopub.status.busy": "2024-10-18T04:23:16.845722Z",
          "iopub.status.idle": "2024-10-18T04:23:17.647144Z",
          "shell.execute_reply": "2024-10-18T04:23:17.646364Z",
          "shell.execute_reply.started": "2024-10-18T04:23:16.846053Z"
        },
        "trusted": true,
        "id": "TvmvG0-gsIUh"
      },
      "outputs": [],
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-17T16:21:47.348875Z",
          "iopub.status.busy": "2024-10-17T16:21:47.348442Z",
          "iopub.status.idle": "2024-10-18T04:07:40.873900Z",
          "shell.execute_reply": "2024-10-18T04:07:40.872833Z",
          "shell.execute_reply.started": "2024-10-17T16:21:47.348842Z"
        },
        "trusted": true,
        "id": "QVTpFVVosIUh",
        "outputId": "0705df2a-453b-4e9a-aace-38550c304fc3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:35<00:00,  3.83s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:56<00:00,  2.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 5.816\n",
            "\t Val. Loss: 5.267\n",
            "Model saved to 'best_model.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:44<00:00,  3.84s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:54<00:00,  2.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02\n",
            "\tTrain Loss: 3.824\n",
            "\t Val. Loss: 4.198\n",
            "Model saved to 'best_model.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:38<00:00,  3.83s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:55<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03\n",
            "\tTrain Loss: 2.741\n",
            "\t Val. Loss: 3.482\n",
            "Model saved to 'best_model.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:34<00:00,  3.83s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:54<00:00,  2.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04\n",
            "\tTrain Loss: 2.100\n",
            "\t Val. Loss: 3.015\n",
            "Model saved to 'best_model.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:37<00:00,  3.83s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:55<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05\n",
            "\tTrain Loss: 1.684\n",
            "\t Val. Loss: 1.689\n",
            "Model saved to 'best_model.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:39<00:00,  3.83s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:55<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06\n",
            "\tTrain Loss: 1.398\n",
            "\t Val. Loss: 1.446\n",
            "Model saved to 'best_model.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:39<00:00,  3.83s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:55<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07\n",
            "\tTrain Loss: 1.179\n",
            "\t Val. Loss: 1.223\n",
            "Model saved to 'best_model.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:43<00:00,  3.84s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:55<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08\n",
            "\tTrain Loss: 1.019\n",
            "\t Val. Loss: 1.084\n",
            "Model saved to 'best_model.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:40<00:00,  3.83s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:54<00:00,  2.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09\n",
            "\tTrain Loss: 0.885\n",
            "\t Val. Loss: 0.938\n",
            "Model saved to 'best_model.pth'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1044/1044 [1:06:41<00:00,  3.83s/it]\n",
            "Evaluating: 100%|██████████| 116/116 [03:54<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10\n",
            "\tTrain Loss: 0.762\n",
            "\t Val. Loss: 0.851\n",
            "Model saved to 'best_model.pth'\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f}')\n",
        "\n",
        "    # Save model if validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({'model_state_dict': model.state_dict(), 'vocab': vocab}, 'best_model.pth')\n",
        "        print(f\"Model saved to 'best_model.pth'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:23:23.563108Z",
          "iopub.status.busy": "2024-10-18T04:23:23.562732Z",
          "iopub.status.idle": "2024-10-18T04:23:23.568624Z",
          "shell.execute_reply": "2024-10-18T04:23:23.567655Z",
          "shell.execute_reply.started": "2024-10-18T04:23:23.563071Z"
        },
        "trusted": true,
        "id": "JHIlKso4sIUh"
      },
      "outputs": [],
      "source": [
        "# Load model function\n",
        "def load_model(filepath, device):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    vocab = checkpoint['vocab']\n",
        "    model = BiLSTMSummarizer(len(vocab), embedding_dim=128, hidden_dim=256, output_dim=len(vocab)).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model, checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoH5-ZVusIUh"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:26:00.423512Z",
          "iopub.status.busy": "2024-10-18T04:26:00.422879Z",
          "iopub.status.idle": "2024-10-18T04:35:46.392752Z",
          "shell.execute_reply": "2024-10-18T04:35:46.391841Z",
          "shell.execute_reply.started": "2024-10-18T04:26:00.423468Z"
        },
        "trusted": true,
        "id": "_T5iMyaCsIUh",
        "outputId": "d223f7f9-92cc-4519-c493-ea20d82d7b14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_48367/67957849.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "best_model, _ = load_model('best_model.pth', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MlKqG8XsIUh"
      },
      "outputs": [],
      "source": [
        "test_loss = evaluate(best_model, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:36:44.635139Z",
          "iopub.status.busy": "2024-10-18T04:36:44.634425Z",
          "iopub.status.idle": "2024-10-18T04:36:44.650051Z",
          "shell.execute_reply": "2024-10-18T04:36:44.649127Z",
          "shell.execute_reply.started": "2024-10-18T04:36:44.635096Z"
        },
        "trusted": true,
        "id": "UL3Jwx52sIUh"
      },
      "outputs": [],
      "source": [
        "def beam_search(model, src, vocab, inv_vocab, beam_width=3, max_length=100, min_length=10, device='cpu'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Embedding the input sequence\n",
        "        embedded = model.embedding(src)  # shape: (batch_size, seq_len, embedding_dim)\n",
        "        enc_output, (hidden, cell) = model.encoder(embedded)  # LSTM encoder output\n",
        "\n",
        "        # In case of bi-directional LSTM, combine the hidden states\n",
        "        if model.encoder.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # shape: (batch_size, hidden_dim)\n",
        "            cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1)        # shape: (batch_size, hidden_dim)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]  # Take the last layer if not bi-directional\n",
        "            cell = cell[-1, :, :]      # Take the last layer if not bi-directional\n",
        "\n",
        "        # Now we process one sequence at a time, so set batch size to 1\n",
        "        hidden = hidden.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
        "        cell = cell.unsqueeze(0)      # shape: (1, batch_size, hidden_dim)\n",
        "\n",
        "        # Initialize the beam with the start-of-sequence token\n",
        "        beam = [([vocab['<sos>']], 0, hidden[:, 0:1, :], cell[:, 0:1, :])]  # Start with one sequence\n",
        "        complete_hypotheses = []\n",
        "\n",
        "        # Perform beam search\n",
        "        for t in range(max_length):\n",
        "            new_beam = []\n",
        "            for seq, score, hidden, cell in beam:\n",
        "                # If end-of-sequence token is reached and length is >= min_length, add to complete hypotheses\n",
        "                if seq[-1] == vocab['<eos>'] and len(seq) >= min_length:\n",
        "                    complete_hypotheses.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Prepare the input for the decoder (last predicted token)\n",
        "                input = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)  # shape: (1, 1)\n",
        "                input_embedded = model.embedding(input)  # shape: (1, 1, embedding_dim)\n",
        "\n",
        "                # Pass through the decoder\n",
        "                output, (hidden, cell) = model.decoder(input_embedded, (hidden, cell))  # Decode step\n",
        "                predictions = model.fc(output.squeeze(1))  # Linear layer to get vocab distribution\n",
        "\n",
        "                # Get top beam_width predictions\n",
        "                topk_scores, topk_indices = torch.topk(predictions, beam_width, dim=1)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    next_seq = seq + [topk_indices[0, i].item()]\n",
        "                    next_score = score + topk_scores[0, i].item()\n",
        "                    new_beam.append((next_seq, next_score, hidden, cell))\n",
        "\n",
        "            # Sort the beam by score and select the top candidates\n",
        "            beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        # If no complete hypotheses were found, return the highest scoring incomplete hypothesis\n",
        "        if len(complete_hypotheses) == 0:\n",
        "            complete_hypotheses = beam\n",
        "\n",
        "        # Return the sequence with the highest score\n",
        "        best_hypothesis = max(complete_hypotheses, key=lambda x: x[1])[0]\n",
        "        return [inv_vocab[idx] for idx in best_hypothesis if idx not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:36:48.003562Z",
          "iopub.status.busy": "2024-10-18T04:36:48.002383Z",
          "iopub.status.idle": "2024-10-18T04:37:30.837693Z",
          "shell.execute_reply": "2024-10-18T04:37:30.836821Z",
          "shell.execute_reply.started": "2024-10-18T04:36:48.003504Z"
        },
        "trusted": true,
        "id": "SNdFRNnGsIUi",
        "outputId": "7d421d20-d8f4-4bc0-f081-489bcfa175e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating summaries: 100%|██████████| 290/290 [00:42<00:00,  6.77it/s]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate ROUGE score\n",
        "rouge = Rouge()\n",
        "best_model.eval()\n",
        "predictions = []\n",
        "references = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n",
        "        src, trg = batch\n",
        "        src = src.to(device)\n",
        "        pred = beam_search(best_model, src, vocab, inv_vocab, min_length=10, device=device)\n",
        "        predictions.extend([' '.join(pred)])\n",
        "        references.extend([' '.join([inv_vocab[idx.item()] for idx in trg[0] if idx.item() not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-18T04:37:40.394028Z",
          "iopub.status.busy": "2024-10-18T04:37:40.393640Z",
          "iopub.status.idle": "2024-10-18T04:37:40.478545Z",
          "shell.execute_reply": "2024-10-18T04:37:40.477666Z",
          "shell.execute_reply.started": "2024-10-18T04:37:40.393991Z"
        },
        "trusted": true,
        "id": "ZBZabeW0sIUi",
        "outputId": "af0c599a-7b5c-4608-8ab3-1619508d70a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE scores: {'rouge-1': {'r': 0.7329, 'p': 0.9464, 'f': 0.9608}, 'rouge-2': {'r': 0.8537, 'p': 0.7654, 'f': 0.8744}, 'rouge-l': {'r': 0.7329, 'p': 0.9465, 'f': 0.9609}}\n"
          ]
        }
      ],
      "source": [
        "min_length = 10\n",
        "predictions = [' '.join(pred[:min_length]) for pred in predictions]\n",
        "scores = rouge.get_scores(predictions, references, avg=True)\n",
        "print(\"ROUGE scores:\", scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECIqeNPfsIUi"
      },
      "outputs": [],
      "source": [
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_9K_ImdsIUi"
      },
      "outputs": [],
      "source": [
        "!jupyter nbconvert --to PDF \"drive/My drive/Colab Notebooks/nlp_assignment_4.ipynb\""
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5867330,
          "sourceId": 9614754,
          "sourceType": "datasetVersion"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 141113,
          "modelInstanceId": 117876,
          "sourceId": 139206,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}